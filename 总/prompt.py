import os
import torch
from PIL import Image

from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig
import warnings

warnings.filterwarnings("ignore")
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

MODEL_PATH = r"C:\Users\HP\.cache\modelscope\hub\models\Qwen\Qwen2___5-VL-7B-Instruct"
IMAGE_DIR = r"D:\PythonProject2\æ€»\clean_images"
OUT_DIR = "captions3"
os.makedirs(OUT_DIR, exist_ok=True)


quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

print("æ­£åœ¨åˆå§‹åŒ– Qwen2.5-VL æ¨¡å‹...")

# 2. é™åˆ¶åˆ†è¾¨ç‡ï¼šQwen2.5-VL é»˜è®¤åƒç´ æé«˜ï¼Œå¿…é¡»é™åˆ¶æ‰èƒ½åœ¨ 8GB æ˜¾å­˜è¿è¡Œ
# min_pixels å’Œ max_pixels è®¾ç½®ä¸º 28 çš„å€æ•°
processor = AutoProcessor.from_pretrained(
    MODEL_PATH,
    min_pixels=256*28*28,
    max_pixels=512*28*28,
    trust_remote_code=True, use_fast=False
)
print("--- æ­¥éª¤ 2: æ­£åœ¨åŠ è½½å¤§æ¨¡å‹åˆ°æ˜¾å­˜ (å¯èƒ½ä¼šæŒç»­ 1-3 åˆ†é’Ÿ) ---")
# 3. åŠ è½½æ¨¡å‹ï¼šä½¿ç”¨ AutoModelForImageTextToText
model = AutoModelForImageTextToText.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    dtype=torch.bfloat16,
    quantization_config=quantization_config,
    trust_remote_code=True
)
print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼æ˜¾å¡å·²å‡†å¤‡å°±ç»ªã€‚")
SYSTEM_PROMPT = ("""
# Role
You are an expert AI image captioning assistant. Your task is to describe the uploaded image for LoRA training.

# Instructions
Describe the image using concise, comma-separated tags. 
The tags must follow this specific order:
1. Start with the trigger phrase: "linkclick_style".
2. Followed by general style tags: "flat color, bold lines, high contrast, anime style".
3. Then describe objective elements: subject (e.g., boy/girl), clothing, hair color, pose, and facial expression.
4. End with background and lighting: (e.g., simple background, blue lighting).

# Constraints
- Use English only.
- Use simple nouns and short phrases.
- Avoid full sentences. 
- Ensure the output starts with "linkclick_style, flat color, bold lines," for every image.
""")
image_files = [f for f in os.listdir(IMAGE_DIR) if f.lower().endswith((".png", ".jpg", ".jpeg", ".webp"))]

print(f"--- æ­¥éª¤ 3: å¼€å§‹å¤„ç†å›¾ç‰‡ (å…± {len(image_files)} å¼ ) ---")
for fname in os.listdir(IMAGE_DIR):
    if not fname.lower().endswith((".png", ".jpg", ".jpeg", ".webp")):
        continue

    image_path = os.path.join(IMAGE_DIR, fname)
    try:
        image = Image.open(image_path).convert("RGB")
        print(f"ğŸš€ æ­£åœ¨åˆ†æ: {fname}")

        # æ„å»ºç¬¦åˆ Qwen2.5 æ ‡å‡†çš„å¯¹è¯
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": SYSTEM_PROMPT},
                ],
            }
        ]

        # å‡†å¤‡è¾“å…¥
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = processor(text=[text], images=[image], padding=True, return_tensors="pt").to(model.device)

        # ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = model.generate(**inputs, max_new_tokens=256)
            # è‡ªåŠ¨è£å‰ªæ‰ Prompt éƒ¨åˆ†
            generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]
            caption = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]

        # ä¿å­˜
        with open(os.path.join(OUT_DIR, os.path.splitext(fname)[0] + ".txt"), "w", encoding="utf-8") as f:
            f.write(caption.strip())

        print(f"âœ… æˆåŠŸ: {caption[:50]}...")

    except Exception as e:
        print(f"âŒ å‡ºé”™: {fname} - {e}")

print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")